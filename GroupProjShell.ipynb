{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Import in the various packages that are used in the code below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import PyPDF2 #Reads PDF Files\n",
    "import sys #System functions \n",
    "import collections #Collections\n",
    "from collections import Counter\n",
    "import urllib2 #open web URLs *newer version\n",
    "from StringIO import StringIO #Handles some exceptions for downloading files from URLs\n",
    "from django.utils.encoding import smart_str, smart_unicode #another method for encoding/decoding ASCII/UTF\n",
    "import datetime #datetime.now() used to track start/finish of code to check processing time\n",
    "import pandas as pd #pandas data-frames(tables)\n",
    "import csv as csv #import/export CSV files\n",
    "from __future__ import print_function #improves printing out PANDAS for testing their content\n",
    "import hashlib #create hash of the binary of each file to match files quickly \n",
    "import os #allows for an easy pull of the file size from local machine\n",
    "import json #JSON objects to store dictionaries to disk when memory becomes a challenge\n",
    "from bs4 import BeautifulSoup #Web Scraping\n",
    "import urllib #open web URLs\n",
    "import re #regular expressions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer #TFIDF Document Comparison\n",
    "from sklearn.metrics.pairwise import cosine_similarity #Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_distances #Cosine Distance\n",
    "from nltk.stem.lancaster import LancasterStemmer #Reduce Words back to their stems\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.manifold import MDS #multidimensional scaling\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "import xlsxwriter\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert unicode to standard encoding\n",
    "def convertU(data):\n",
    "    if isinstance(data, basestring):\n",
    "        return smart_str(data)\n",
    "    elif isinstance(data, collections.Mapping):\n",
    "        return dict(map(convertU, data.iteritems()))\n",
    "    elif isinstance(data, collections.Iterable):\n",
    "        return type(data)(map(convertU, data))\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "#Converts the typical output for a PyPDF2 Metadata Date into a useable format. \n",
    "def convertPyPDFDate(origDate):\n",
    "    if len(origDate) == 23:\n",
    "        year = origDate[2:6]\n",
    "        mon = origDate[6:8]\n",
    "        day = origDate[8:10]\n",
    "        return (year+\"-\"+mon+\"-\"+day)\n",
    "    return origDate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Directory of original .PDF File URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creates Tab-Separated File (there are commas in the content) of the metadata for reference\n",
    "def recreateTsvFile():\n",
    "    urldf = pd.read_csv(\"PDFs.csv\")  #Import List of PDF Files from CSV\n",
    "    urldf[\"URL\"][:10] #first 10 rows of column \"URL\"\n",
    "    urldf[\"URL\"].value_counts() #visual check for duplicates\n",
    "    urldf[\"UID\"] = \"\"  \n",
    "    urldf[\"Download\"] = \"\" \n",
    "    urldf[\"CreatedDate\"] = \"\"\n",
    "    urldf[\"ModDate\"] = \"\"\n",
    "    urldf[\"Creator\"] = \"\"\n",
    "    urldf[\"Producer\"] = \"\"\n",
    "    urldf[\"Author\"] = \"\"\n",
    "    urldf[\"Title\"] = \"\"\n",
    "\n",
    "    for x in range (0, len(urldf)):\n",
    "        urldf[\"UID\"][x] = \"BSA_\"+str(x+101)  #create UID for each PDF FIle\n",
    "\n",
    "    urldf.to_csv(\"D:\\BSA_PDF_Files\\URLs_Updated.txt\", sep=\"\\t\") #write to new csv file the URL & UID for each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#imput is a preformatted Data frame of the URLs and Unique Identifier (UID) for each file\n",
    "def downloadPdfAsIs(urldf):\n",
    "\n",
    "    folder = \"D:\\BSA_PDF_Files\\\\\"\n",
    "    \n",
    "    #hdr tricks the server into thinking the file is being accessd from Mozilla firefox\n",
    "    #this is neccisary because some servers block the traffic without this header\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "\n",
    "    #loop through all of the URLs in the passed data frame \n",
    "    for x in range(0, len(urldf)):\n",
    "        try: #try was neccisary because ~1% of files failed to download.  this prevents process from crashing\n",
    "            if pd.isnull(urldf[\"Download\"][x]): #the \"Download\" field was used to track prior successes and allow me to break the data frame up and process in chunks\n",
    "                tempURL = str(urldf[\"URL\"][x]) #retrieve URL from the data frame\n",
    "                tempReq = urllib2.Request(tempURL, headers=hdr) #create request object with urllib2 to open the URL\n",
    "                try: \n",
    "                    tempPage = urllib2.urlopen(tempReq) #attempt to open the URL and capture the page content (PDF file)\n",
    "                    urldf[\"Download\"][x] = \"Success\" #if download worked, write \"Success\" in the \"Download\" column\n",
    "                except urllib2.HTTPError, e: \n",
    "                    urldf[\"Download\"][x] = e.fp.read() #on error, write the error under the \"Download\" column\n",
    "                    continue\n",
    "                tempPdf = PyPDF2.PdfFileReader(StringIO(tempPage.read())) #create PDF file object from the original URL, using the StringIO function to correct for some errors\n",
    "            \n",
    "                if tempPdf.isEncrypted: #check and see if the file is thought to be encrypted\n",
    "                    try: tempPdf.decrypt('') #if we think its encrypted, attempt to decrypt without a password\n",
    "                    except: print(\"Attempted decrpytion failed on line: \"+str(x)) #if the decrpytion failed, print to console so the operator realizes and can react\n",
    "            \n",
    "                merger = PyPDF2.PdfFileMerger() #create a blank shell of a PDF file that will eventually be merged with the downloaded content\n",
    "                try: merger.append(tempPdf) #attempt to add the downloaded content to the blank pdf we just created\n",
    "                except:\n",
    "                    urldf[\"Download\"][x]=\"Outfile Error: No Content in File\"+str(x) #if the merge of the content fails, write error message to table\n",
    "                    continue\n",
    "            \n",
    "                try: docInfoLen = len(tempPdf.documentInfo) #check and see if the original (URL) PDF file has metadata(documentInfo) that we can detect\n",
    "                except:\n",
    "                    urldf[\"Download\"][x]=\"Encrpytion Error: Couldnt Decrypt Metadata on File\"+str(x) #There always is metadata, so if we cant see any, it relates to failing to decrypt the file\n",
    "                    continue\n",
    "            \n",
    "                if docInfoLen>0: #if we have metadata... \n",
    "                    try:merger.addMetadata(tempPdf.documentInfo) #try to add the metadata to the PDF shell that we previously added the content to\n",
    "                    except: urldf[\"Download\"][x]=\"Metadata error: Metadata not written to file\"+str(x) #write error to table if this failed\n",
    "                    #all of these similar formatted try/excepts are to write the metadata out to the table/data-frame that we have been working from\n",
    "                    try: urldf[\"CreatedDate\"][x] = convertPyPDFDate(tempPdf.documentInfo[\"/CreationDate\"])\n",
    "                    except: urldf[\"CreatedDate\"][x] = \"\"\n",
    "                \n",
    "                    try: urldf[\"ModDate\"][x] = convertPyPDFDate(tempPdf.documentInfo[\"/ModDate\"])\n",
    "                    except: urldf[\"ModDate\"][x] = \"\"\n",
    "            \n",
    "                    try: urldf[\"Creator\"][x] = tempPdf.documentInfo[\"/Creator\"]\n",
    "                    except: urldf[\"Creator\"][x] = \"\"\n",
    "            \n",
    "                    try: urldf[\"Producer\"][x] = tempPdf.documentInfo[\"/Producer\"]\n",
    "                    except:urldf[\"Producer\"][x] = \"\"\n",
    "            \n",
    "                    try: urldf[\"Author\"][x] = tempPdf.documentInfo[\"/Author\"]\n",
    "                    except:urldf[\"Author\"][x] = \"\"\n",
    "                \n",
    "                    try: urldf[\"Title\"][x] = tempPdf.documentInfo[\"/Title\"]\n",
    "                    except:urldf[\"Title\"][x] = \"\"\n",
    "        \n",
    "        \n",
    "                outFile= open(folder+urldf[\"UID\"][x]+\".pdf\", 'wb') #open a new PDF file on the drive to write the merged content and metadata to \n",
    "                try: merger.write(outFile) #try to write the file\n",
    "                except: \n",
    "                    outFile.write(\"Error writing file. Please Manually Download\") #if writing fails, annotate it in the table/dataframe\n",
    "                    urldf[\"Download\"][x]=\"Outfile Error: No Content in File\"+str(x)\n",
    "                outFile.close()\n",
    "            else:\n",
    "                #print(str(x), end=\", \")\n",
    "                continue #for any URL/UID combo where we already wrote in the \"Download\" field, skip it and move to the next one (helped me run it in batches)\n",
    "        except:\n",
    "            urldf.to_csv(\"D:\\BSA_PDF_Files\\URLs_Updated.txt\",sep=\"\\t\",header=True, index=False, encoding='utf-8') #when we have errors, write anything we have in the table/data-frame in memory to disk, and continue working\n",
    "    urldf.to_csv(\"D:\\BSA_PDF_Files\\URLs_Updated.txt\",sep=\"\\t\",header=True, index=False, encoding='utf-8') #write the table/data-frame out as a tab-sep spreadsheet\n",
    "\n",
    "############################    \n",
    "### Run Download Process ###\n",
    "############################\n",
    "#recreateTsvFile() #recreate the TSV file/data-frame that we work from, which re-starts the process over again (I used this in the original design/testing of the project)\n",
    "#urldf = pd.read_csv(\"D:\\BSA_PDF_Files\\URLs_Updated.txt\", sep=\"\\t\") #read the master tsv file into memory that we are working from.  If we dont use the \"recreate\" function, it resumes from the last time it wrote to disk\n",
    "#downloadPdfAsIs(urldf) #ready... set... download 37,000+ files!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create JSON Object for Each Downloaded PDF\n",
    "And Create HASH of each file's binary content to compare for exact duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create a individaul dictionary for each UID/URL Pairs and write to disk as a JSON object named after the file\n",
    "#also create a master dictionary with each hashed value as the key, and the file(s) that match that hash as the values (in a list of UID strings)\n",
    "def createJSON(uid): #process one UID as a time because Dictionarys are immutable and we cannot write to the same name dictionary twice in one iteration\n",
    "    pdfDict = {} #blank dictionary to add data to\n",
    "    temp_pdf = PyPDF2.PdfFileReader(file('D:\\BSA_PDF_Files\\\\'+uid+'.pdf',\"rb\")) #open the PDF file that we have previously saved to disk as a binary object\n",
    "    \n",
    "    #below is the process to write each key:value pair into the dictionary for that file\n",
    "    #2.b. UID:String\n",
    "    pdfDict.update({'UID':uid})\n",
    "    \n",
    "    #2.c. URL:String\n",
    "    pdfDict.update({'URL':urlref[uid]}) #pulls from the dictionary outside of this function\n",
    "    \n",
    "    #2.d. Metadata:Dictionary\n",
    "    pdfDict.update({'Metadata':temp_pdf.documentInfo})\n",
    "    \n",
    "    #2.e. File_Size:long\n",
    "    pdfDict.update({'File_Size': os.path.getsize('D:\\BSA_PDF_Files\\\\'+uid+'.pdf')}) #uses the OS import to get the file size\n",
    "    \n",
    "    #2.f. Hash:String \n",
    "    tempHash = hashlib.sha1(open('D:\\BSA_PDF_Files\\\\'+uid+'.pdf','rb').read()).hexdigest() #using the sha1 hashing algorithm, has the binary of the file and write to a temporary string value\n",
    "    pdfDict.update({'Hash':tempHash}) #add the hash to my file-specific dictionary\n",
    "    #add has to global dictionary if not already present, else append UID to the hash\n",
    "    global glbHashes #open global hash dictionary (declared lower in the code, but before this function is called)\n",
    "    if tempHash in glbHashes:\n",
    "        glbHashes[tempHash].append(uid) #add new has to global dictionary\n",
    "    else:\n",
    "        glbHashes.update({tempHash:[uid]}) #append UID to an existing list of UIDs for that hash value\n",
    "    \n",
    "    \n",
    "    url = urlref[uid].strip().split('/') #break the original URL on the '/' character to determine the domain, folder and file name from the original URL\n",
    "    #2.g. Domain:String\n",
    "    pdfDict.update({'Domain':url[2]})\n",
    "    \n",
    "    #2.i. Folder: String\n",
    "    pdfDict.update({'Folder':url[-2]})\n",
    "    \n",
    "    #2.j. fileName:String\n",
    "    pdfDict.update({'File_Name':url[-1]})\n",
    "    \n",
    "    with open('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json','w') as j: #write the tempoary dictionary for the individual PDF file to disk.\n",
    "        json.dump(pdfDict, j, indent = 4)\n",
    "\n",
    "#################################\n",
    "### Run JSON Creation Process ###\n",
    "#################################        \n",
    "#Import/create object with UIDs and URLs to use as the starting point for the dictionaries\n",
    "def createMyJSONobjs():\n",
    "    urlref = {} #blank dictionary to work from \n",
    "    uids = [] #list of UIDs that we need to iterate thought\t\n",
    "    urldf = pd.read_csv(\"UID_URL_List.txt\", sep='\\t') #read the file to memeory\n",
    "    for x in range (0, len(urldf)): \n",
    "        urlref.update({urldf[\"UID\"][x]:urldf[\"URL\"][x]}) #create dictionary of UID:URL matching to pull URL and add to JSON for the file\n",
    "        uids.append(urldf[\"UID\"][x]) #create list of UIDs that we are going to loop over\n",
    "\n",
    "    glbHashes = {} #blank dictionary for the global hash:uid dictionary\n",
    "    #Iterate through each and every UID to create a new dictionary for that file:\n",
    "    for uid in uids:\n",
    "        try: createJSON(uid) #create exception in case of errors, so the process doesnt fail\n",
    "        except: print(\"Failure on UID: \"+uid) #less than 0.01% of files had an error in the JSON creation process\n",
    "    #write the global hashes out as a JSON object as well to store it to disk\n",
    "    with open ('D:\\BSA_PDF_Files\\JSON\\hashMatching.json','w') as f:\n",
    "        json.dump(glbHashes, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create XLSX Workbook to view perfect duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createHashMatchXLSX():\n",
    "    with open('D:\\BSA_PDF_Files\\JSON\\hashMatching.json', 'r') as f: #open JSON object of Hash:[UIDs] we just created\n",
    "         hashMatches = json.load(f)\n",
    "\n",
    "    workbook = xlsxwriter.Workbook('duplicates.xlsx') #create new XLSX file\n",
    "    worksheet = workbook.add_worksheet() #create new worksheet in that XLSX file\n",
    "    x=1 #easy loop value/iterator \n",
    "    worksheet.write('A1',\"Hash\") #add name to first column\n",
    "    worksheet.write('B1',\"UIDs\") #add name to second column that identifies all following columns as well\n",
    "    uniqueUIDs = [] #blank list to start adding the first UID of any unique hash to\n",
    "\n",
    "    for key, values in hashMatches.iteritems(): #iterate through dictionary on key:value pairs\n",
    "        worksheet.write(x,0,key) #write the key to the first column of the row as determined by x (0 = row 1 in Excel, so starting from x=1 starts writing data at row 2, with labels in row 1)\n",
    "        uniqueUIDs.append(values[0]) #add that first (or only) UID to the list of unique files\n",
    "        for value in values: #iterate through list of UIDs associated with the hash Key\n",
    "            worksheet.write(x,(values.index(value)+1),value) #write the UIDs in columns, as many columns as needed for the number of UIDs associated with that key\n",
    "        x+=1    #iterate x so that I always write to the next line\n",
    "    workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull Textual Content & Page Numbers\n",
    "Add both to the existing dictionaries for the unique files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uniqueUIDs = (pd.read_csv('UniqueFileUIDs.csv'))['UID'].tolist() #create list of unique(hash) UIDs to work off of\n",
    "\n",
    "def convertPDF2String(path):\n",
    "    content = \"\"\n",
    "    # load pdf file\n",
    "    pdf = PyPDF2.PdfFileReader(file(path,\"rb\"))\n",
    "    if pdf.isEncrypted: \n",
    "        try: pdf.decrypt('')\n",
    "        except: print(\"Attempted decrpytion failed on UID: \"+uid)\n",
    "    # iterate pages\n",
    "    for i in range(0, pdf.getNumPages()):\n",
    "        # extract the text from each page\n",
    "        try: content += pdf.getPage(i).extractText() + \" \\n\"\n",
    "        except: continue\n",
    "    # collapse whitespaces\n",
    "    content = \" \".join(content.replace(u\"\\xa0\", \" \").split()).encode('utf-8')\n",
    "    return (content) #.encode('utf-8')\n",
    "\n",
    "#Append textual content to dictionary for ecah unique PDF file\n",
    "def contentReadAndAppend(uid):\n",
    "    try: \n",
    "        with open('D:\\BSA_PDF_Files\\JSON2\\\\'+uid+'.json', 'r') as i:\n",
    "            uid_dict_check = json.load(i)\n",
    "            old_content = uid_dict_check[\"Content\"]\n",
    "    except: \n",
    "        old_content = ''\n",
    "    if len(old_content)==0:\n",
    "        content = convertPDF2String('D:\\BSA_PDF_Files\\\\'+uid+'.pdf')\n",
    "        with open('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json', 'r') as j: #open JSON for the file in question\n",
    "            uid_dict = json.load(j)\n",
    "        uid_dict.update({\"Content\":content})\n",
    "        with open('D:\\BSA_PDF_Files\\JSON2\\\\'+uid+'.json', 'w') as k:\n",
    "            json.dump(uid_dict,k,indent=4)\n",
    "\n",
    "#for uid in uniqueUIDs:\n",
    "#    try:contentReadAndAppend(uid)\n",
    "#    except:print(\"Error on UID: \"+uid)\n",
    "\n",
    "def addPageNumbers(uid):\n",
    "    try:\n",
    "        with open ('D:\\BSA_PDF_Files\\JSON2\\\\'+uid+'.json','r') as j:\n",
    "            uid_dict_check = json.load(i)\n",
    "            old_pgn = uid_dict_check[\"Pages\"]\n",
    "    except:\n",
    "        old_pgn = 0\n",
    "    if old_pgn==0:\n",
    "        tempPdf = PyPDF2.PdfFileReader(file('D:\\BSA_PDF_Files\\\\'+uid+'.pdf','rb'))\n",
    "        if tempPdf.isEncrypted: \n",
    "                    try: tempPdf.decrypt('')\n",
    "                    except: print(\"Attempted decrpytion failed on uid: \"+uid)\n",
    "        pageNumbers = tempPdf.getNumPages()\n",
    "        #print (pageNumbers)\n",
    "        with open('D:\\BSA_PDF_Files\\JSON2\\\\'+uid+'.json', 'r') as k:\n",
    "            uid_dict = json.load(k)\n",
    "        uid_dict.update({\"Pages\":pageNumbers})\n",
    "        with open('D:\\BSA_PDF_Files\\JSON2\\\\'+uid+'.json', 'w') as l:\n",
    "            json.dump(uid_dict,l,indent=4)\n",
    "            \n",
    "#for uid in uniqueUIDs:\n",
    "#    try:addPageNumbers(uid)\n",
    "#    except:print(\"Error on UID: \"+uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scrape Website for \"Language of Scouting\"\n",
    "Will be used to tag documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajwil\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 174 of the file C:\\Users\\ajwil\\Anaconda2\\lib\\runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "r = urllib.urlopen('http://www.scouting.org/scoutsource/Media/LOS/All.aspx').read()\n",
    "soup = BeautifulSoup(r)\n",
    "#print type(soup)\n",
    "#print soup.prettify()[0:100000]\n",
    "\n",
    "#from IPython.display import HTML\n",
    "#HTML('<iframe src=http://www.scouting.org/scoutsource/Media/LOS/All.aspx width=1000 height=500></iframe>')\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleanr = re.compile('<.*?>')\n",
    "  cleantext = re.sub(cleanr, '', raw_html)\n",
    "  return cleantext\n",
    "\n",
    "#Find from <strong> </strong> tags\n",
    "tds = soup.find_all(\"td\", {\"valign\": \"top\"})\n",
    "\n",
    "BSA_Words_Long = []\n",
    "BSA_Alternating_Words = []\n",
    "x=0\n",
    "\n",
    "for td in tds:\n",
    "    try:\n",
    "        word= td.find_all(\"strong\")\n",
    "        if len(word)>0:\n",
    "            BSA_Words_Long.append(cleanhtml(str(word[0])[8:-9].decode('utf-8')))\n",
    "            if x%2 == 0:\n",
    "                BSA_Alternating_Words.append(cleanhtml(str(word[0])[8:-9].decode('utf-8')))\n",
    "                print(cleanhtml(str(word[0])[8:-9].decode('utf-8')))\n",
    "        x+=1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "with open ('BSA_Dictionary2.txt','w') as e:\n",
    "    for item in BSA_Words_Long:\n",
    "        try:e.write(\"%s\\n\" % item).decode('utf-8')\n",
    "        except: continue\n",
    "\n",
    "with open ('BSA_Dictionary1.txt','w') as e:\n",
    "    for item in BSA_Alternating_Words:\n",
    "        try:e.write(\"%s\\n\" % item).decode('utf-8')\n",
    "        except: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Master Repository of Content\n",
    "### Starting from the complete PDF Library being downloaded and the binary file of each PDF document hashed and appended to a JSON object for that file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recreate complete list of JSON objects that were created:\n",
    "working_uid_list = []\n",
    "working_hash_dict = {}\n",
    "\n",
    "for x in range (100, 37871):\n",
    "    try:\n",
    "        with open ('D:\\BSA_PDF_Files\\JSON\\BSA_{}.json'.format(x),'r') as j:\n",
    "            temp_hash = ujson.load(j)[\"Hash\"]\n",
    "            working_uid_list.append('BSA_{}'.format(x))\n",
    "            if temp_hash in working_hash_dict:\n",
    "                working_hash_dict[temp_hash].append('BSA_{}'.format(x))\n",
    "            else:\n",
    "                working_hash_dict.update({temp_hash:['BSA_{}'.format(x)]})\n",
    "    except:\n",
    "        continue\n",
    "print(len(working_uid_list))\n",
    "print(len(working_hash_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertPDF2String(path):\n",
    "    content = \"\"\n",
    "    # load pdf file\n",
    "    pdf = PyPDF2.PdfFileReader(file(path,\"rb\"))\n",
    "    if pdf.isEncrypted: \n",
    "        try: pdf.decrypt('')\n",
    "        except: print(\"Attempted decrpytion failed on UID: \"+uid)\n",
    "    # iterate pages\n",
    "    for i in range(0, pdf.getNumPages()):\n",
    "        # extract the text from each page\n",
    "        try: content += pdf.getPage(i).extractText() + \" \\n\"\n",
    "        except: continue\n",
    "    # collapse whitespaces\n",
    "    content = \" \".join(content.replace(u\"\\xa0\", \" \").split()).encode('utf-8')\n",
    "    return (content) #.encode('utf-8')\n",
    "\n",
    "#Append textual content to dictionary for ecah unique PDF file\n",
    "def contentReadAndAppend(uid):\n",
    "    try: \n",
    "        with open('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json', 'r') as i:\n",
    "            uid_dict_check = ujson.load(i)\n",
    "            old_content = uid_dict_check[\"Content\"]\n",
    "    except: \n",
    "        old_content = None\n",
    "    if old_content is None:\n",
    "        content = convertPDF2String('D:\\BSA_PDF_Files\\\\'+uid+'.pdf')\n",
    "        with open('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json', 'r') as j: #open JSON for the file in question\n",
    "            uid_dict = ujson.load(j)\n",
    "        uid_dict.update({\"Content\":content})\n",
    "        with open('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json', 'w') as k:\n",
    "            ujson.dump(uid_dict,k,indent=4)\n",
    "\n",
    "def addPageNumbers(uid):\n",
    "    try:\n",
    "        with open ('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json','r') as j:\n",
    "            uid_dict_check = ujson.load(i)\n",
    "            old_pgn = uid_dict_check[\"Pages\"]\n",
    "    except:\n",
    "        old_pgn = None\n",
    "    if old_pgn is None:\n",
    "        tempPdf = PyPDF2.PdfFileReader(file('D:\\BSA_PDF_Files\\\\'+uid+'.pdf','rb'))\n",
    "        if tempPdf.isEncrypted: \n",
    "                    try: tempPdf.decrypt('')\n",
    "                    except: print(\"Attempted decrpytion failed on uid: \"+uid)\n",
    "        pageNumbers = tempPdf.getNumPages()\n",
    "        #print (pageNumbers)\n",
    "        with open('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json', 'r') as k:\n",
    "            uid_dict = ujson.load(k)\n",
    "        uid_dict.update({\"Pages\":pageNumbers})\n",
    "        with open('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json', 'w') as l:\n",
    "            ujson.dump(uid_dict,l,indent=4)\n",
    "\n",
    "for uid in working_uid_list:\n",
    "    try:contentReadAndAppend(uid)\n",
    "    except:print(\"Content Error on UID: \"+uid)\n",
    "    try:addPageNumbers(uid)\n",
    "    except:print(\"PageNum Error on UID: \"+uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content_hash_dict = {}\n",
    "\n",
    "def create_content_hash(uid):\n",
    "    global content_hash_dict\n",
    "    try:\n",
    "        with open ('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json','r') as j:    \n",
    "            uid_dict = ujson.load(j)\n",
    "        uid_content = uid_dict['Content']\n",
    "        \n",
    "    except:\n",
    "        uid_content = None    \n",
    "    #print(uid_content)\n",
    "    \n",
    "    if uid_content is not None: \n",
    "        try:\n",
    "            tempHash = hashlib.sha1(uid_content).hexdigest()\n",
    "        except:\n",
    "            decoded_content = unidecode(uid_content)\n",
    "            tempHash = hashlib.sha1(decoded_content).hexdigest()\n",
    "        #print(tempHash)\n",
    "        if tempHash in content_hash_dict:\n",
    "            content_hash_dict[tempHash].append(uid) #add new has to global dictionary\n",
    "        else:\n",
    "            content_hash_dict.update({tempHash:[uid]}) #append UID to an existing list of UIDs for that hash value\n",
    "        \n",
    "        uid_dict.update({\"Content_Hash\":tempHash})\n",
    "\n",
    "    with open('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json','w') as k:\n",
    "        ujson.dump(uid_dict,k,indent=4)\n",
    "\n",
    "\n",
    "for uid in working_uid_list:\n",
    "    create_content_hash(uid)\n",
    "        \n",
    "print(len(content_hash_dict))\n",
    "\n",
    "with open('D:\\BSA_PDF_Files\\JSON\\Content_Hash_Dict.json','w') as o:\n",
    "    ujson.dump(content_hash_dict,o,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize and use MongoDB for the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "# Connect to the mongo local database\n",
    "connection = MongoClient()\n",
    "db = connection.bsa_files\n",
    "collection = db.bsa_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_JSON_to_mongo(uid):\n",
    "    with open('D:\\BSA_PDF_Files\\JSON\\{}.json'.format(uid),'r') as j:\n",
    "        collection.insert_one(ujson.load(j))\n",
    "\n",
    "for uid in working_uid_list:\n",
    "    write_JSON_to_mongo(uid)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
