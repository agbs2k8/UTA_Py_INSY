{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37871\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "current_UID_position = pickle.load(open('current_uid.pickle','r'))\n",
    "print(current_UID_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('current_uid.pickle','w') as q:\n",
    "    pickle.dump(37871, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "# Connect to the mongo local database\n",
    "connection = MongoClient()\n",
    "db = connection.bsa_files\n",
    "collection = db.bsa_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convertPyPDFDate(origDate):\n",
    "    if len(origDate) == 23:\n",
    "        year = origDate[2:6]\n",
    "        mon = origDate[6:8]\n",
    "        day = origDate[8:10]\n",
    "        return (year+\"-\"+mon+\"-\"+day)\n",
    "    return origDate\n",
    "\n",
    "def download_from_url(url):\n",
    "    import urllib2\n",
    "    import PyPDF2\n",
    "    from StringIO import StringIO\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "    \n",
    "    try: \n",
    "        tempReq = urllib2.Request(url, headers=hdr) \n",
    "        try: \n",
    "            tempPage = urllib2.urlopen(tempReq)\n",
    "            print(\"page opened\")\n",
    "        except urllib2.HTTPError, e: \n",
    "            print(e.fp.read())\n",
    "        \n",
    "        tempPdf = PyPDF2.PdfFileReader(StringIO(tempPage.read()))\n",
    "        print(\"temporary PDF file created\")\n",
    "        \n",
    "        if tempPdf.isEncrypted: \n",
    "            try: \n",
    "                tempPdf.decrypt('') \n",
    "                print('pdf file was encrypted')\n",
    "            except: \n",
    "                print(\"Attempted decrpytion failed\") \n",
    "                \n",
    "        merger = PyPDF2.PdfFileMerger()\n",
    "        print(\"merger file created\")\n",
    "        \n",
    "        try: \n",
    "            merger.append(tempPdf)\n",
    "            print(\"temp file appended to merge file\")\n",
    "        except:\n",
    "            print(\"No file content found\") \n",
    "\n",
    "        try: \n",
    "            docInfoLen = len(tempPdf.documentInfo)\n",
    "            print('file length found')\n",
    "        except:\n",
    "            print(\"Encrpytion Error: Couldnt Decrypt Metadata on File\") \n",
    "\n",
    "        if docInfoLen>0: \n",
    "            try:\n",
    "                merger.addMetadata(tempPdf.documentInfo)\n",
    "                print('metadata written to file')\n",
    "            except: \n",
    "                print(\"Metadata not written to file\")\n",
    "        temp_return = StringIO()\n",
    "        merger.write(temp_return)\n",
    "        return (temp_return.getvalue())\n",
    "\n",
    "    except:\n",
    "        print(\"There was an error retrieving the file.  Please check the URL and try again.\")\n",
    "        return None\n",
    "\n",
    "def extract_text_content_and_pages(pdf):\n",
    "    content = \"\"\n",
    "    if pdf.isEncrypted: \n",
    "        try: \n",
    "            pdf.decrypt('')\n",
    "        except: None\n",
    "    # iterate pages\n",
    "    for i in range(0, pdf.getNumPages()):\n",
    "        # extract the text from each page\n",
    "        try: content += pdf.getPage(i).extractText() + \" \\n\"\n",
    "        except: continue\n",
    "    # collapse whitespaces\n",
    "    content = \" \".join(content.replace(u\"\\xa0\", \" \").split()).encode('utf-8')\n",
    "    return (content,pdf.getNumPages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter File Path or URL: C:\\Users\\ajwil\\Desktop\\AJs PS3.pdf\n",
      "{'Content': 'PROBLEM SET 3 ECON 5337 ALEXANDER WILSON 1. To complete the forecasting exercise, there are two packages/libraries that must be loaded: >library(timeSeries) >library(forecast) >library(urca) A. To load the data and plot the time series data, it requires reading the csv file into memory, converting it to a time series with the ts() function, and then plotting the time series function. >data < - read.csv(\"pset3.csv\") >yuemp = ts(data$Unemploy, star t = c(1955,1), end = c(2015,1),freq = 4) >plot(yuemp) B. To forecast the youth unemployment rate, guesses must be made about the ARMA(p,q) structure of the model. To aid in making the guess, the autocorrelation function and partial autocorrelation fun ction should be used in order to look at any possible patterns in the data. >acf(yuemp) PROBLEM SET 3 ECON 5337 ALEXANDER WILSON We of course ignore the value at lag 0 and see multiple statistically significant, positive autocorrelation coefficients, that are moving towards 0. In theory, an AR(2) has an ACF that only approaches 0 in the limit, so this support the hypothesis of this could support this being an AR(2) model. >pacf(yuemp) The partial autocorrelation function graph shows 2 obviously statistically significant partial autocorrela tion coefficients at lag 1 & 2 and four potentially significant partial auto correlation coefficients at lags 3, 7, 9 and 11. autocorrelations approaching 0 in the limit. PROBLEM SET 3 ECON 5337 ALEXANDER WILSON Befor e selecting a model, we also check for stationarity. Using the Dickey - Fuller unit root test, we proceed with the hull hypothesis that the unit root is present and the alternate hypothesis of stationarity. > myts_df_test = ur.df(log(yuemp ), type = \"trend\", lags = 24, selectlags = \"AIC\") > summary(myts_df_test) ############################################### # Augmented Dickey - Fuller Test Unit Root Test # ############################################### Test regression trend Call: lm( formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag) Residuals: Min 1Q Median 3Q Max - 0.088721 - 0.020745 - 0.001633 0.019508 0.112502 Coefficients: Estimate Std. Error t value Pr(>|t|) (Intercept) 1.359e - 01 4.077e - 02 3.333 0.001020 ** z.lag.1 - 5.749e - 02 1.688e - 02 - 3.406 0.000796 *** tt 5.829e - 05 4.239e - 05 1.375 0.170645 z.diff.lag1 3.181e - 01 6.767e - 02 4.701 4.78e - 06 *** z.diff.lag2 3.679e - 01 7.029e - 02 5.235 4.11e - 07 *** z.diff.lag3 7.146e - 02 7.233e - 02 0.988 0.324308 z.diff.lag4 - 1.692e - 01 7.264e - 02 - 2.329 0.020865 * z.diff.lag5 - 8.470e - 02 7.245e - 02 - 1.169 0.243736 z.diff.lag6 2.085e - 01 7.270e - 02 2.867 0.004575 ** z.diff.lag7 1.211e - 01 7.1 62e - 02 1.691 0.092440 . z.diff.lag8 - 2.063e - 01 7.149e - 02 - 2.885 0.004338 ** z.diff.lag9 - 7.438e - 02 7.062e - 02 - 1.053 0.293444 z.diff.lag10 1.758e - 01 6.663e - 02 2.638 0.008987 ** --- Residual standard error: 0.03632 on 203 degrees of freedom Multiple R - squared: 0.3501, Adjusted R - squared: 0.3117 F - statistic: 9.115 on 12 and 203 DF, p - value: 6.01e - 14 Value of test - statistic is: - 3.4055 3.8902 5.8211 Critical values for t est statistics: 1pct 5pct 10pct tau3 - 3.99 - 3.43 - 3.13 phi2 6.22 4.75 4.07 phi3 8.43 6.49 5.47 With a p - value of 6.01e - 14, we reject the null hypothesis and believe the model is stationary. I plan to initially assess the AR(2) and MA(2) models, and with the available computing power also investigate ARMA(1,1), ARMA(1,2), ARMA(2,1), ARMA(2,2), ARMA(2,3) and ARMA(1,3). PROBLEM SET 3 ECON 5337 ALEXANDER WILSON C. Investigation of models. Each model is run through the Arima modeling function in R, and the results are recorded on t he following line. The models with the lowest AIC & SIC (BIC) scores are saved as objects for further processing. AR(2) > Arima(yuemp,order=c(2,0,0)) AICc=373.95 BIC=387.71 MA(2) >Arima(yuemp,order=c(0,0,2)) AICc=666.76 BIC=680.53 ARMA(1,1) >Arima( yuemp,order=c(1,0,1)) AICc=397.37 BIC=411.14 ARMA(1,2) >Arima(yuemp,order=c(1,0,2)) AICc=379.66 BIC=396.83 ARMA(2,1) >arma21 < - Arima(yuemp,order=c(2,0,1)) AICc=371.45 BIC=388.62 ARMA(2,2) >arma22 < - Arima(yuemp,order=c(2,0,2)) AICc=370.1 BIC=390.65 ARMA(2,3) >arma23 < - Arima(yuemp,order=c(2,0,3)) AICc=364.78 BIC=388.69 ARMA(1,3) >arma13 < - Arima(yuemp,order=c(1,0,3)) AICc=362.8 BIC=383.35 The ARMA(2,1), ARMA(2,2) ARMA(2,3) and ARMA(1,3) appear to be the most appealing potential mode ls , and will be further investigated. D. In order to select a model, the autocorrelation of the residuals and the Ljung - Box test were calculated for each of the identified models: ARMA(2,1), ARMA(2,2) ARMA(2,3) and ARMA(1,3) PROBLEM SET 3 ECON 5337 ALEXANDER WILSON ARMA(2,1) >acf (arma21$residuals) This plot shows one potentially statistically significant residual at lag 8, but generally shows residual autocorrelations within the confidence bands. For the Ljung - Box Test, the Null hypothesis (H 0 ) is that the first 20 auto cor relation coefficients are jointly equal to 0 for the model residuals. The alternate hypothesis (H a ) is the complement of the null. > Box.test(arma21$residuals,lag=20, type = \"Ljung - Box\") Box - Ljung test data: arma21$residuals X - squared = 25.58, df = 20, p - value = 0.1801 This P - value means that 18.01% of all 2 random variables are less than test statistic, and we need this to be below 5%, so we will reject the null hypothesis. PROBLEM SET 3 ECON 5337 ALEXANDER WILSON ARMA(1,3) >acf(arma13$residuals) This plot shows one potentially statistically significant residual at lag 8, but generally shows residual autocorrelations within the confidence bands. For the Ljung - Box Test, the Null hypothesis (H 0 ) is that the first 20 auto correlation coefficients are jointly equal to 0 for the model residuals. The alternate hypothesis (H a ) is the complement of the null. > Box.test(arma13$residuals,lag=20, type = \"Ljung - Box\") Box - Ljung test data: arma13$residuals X - squared = 28.501, df = 20, p - value = 0.09805 This P - value means that 9.805% of all 2 random variables are less than test statistic, and we need this to be below 5%, so we will reject the null hypothesis. PROBLEM SET 3 ECON 5337 ALEXANDER WILSON ARMA(2,3) >acf(arma23$residuals) This plot shows one po tentially statistically significant residual at lag 8, but generally shows residual autocorrelations within the confidence bands. For the Ljung - Box Test, the Null hypothesis (H 0 ) is that the first 20 auto correlation coefficients are jointly equal to 0 for the model residuals. The alternate hypothesis (H a ) is the complement of the null. > Box.test(arma23$residuals,lag=20, type = \"Ljung - Box\") Box - Ljung test data: arma23$residuals X - squared = 28.763, df = 20, p - value = 0.09253 This P - v alue means that 9.253% of all 2 random variables are less than test statistic, and we need this to be below 5%, so we will reject the null hypothesis. PROBLEM SET 3 ECON 5337 ALEXANDER WILSON ARMA(2,2) >acf(arma22$residuals) This plot shows two potentially statistically significant residuals at lags 6 & 8 but generally shows residual autocorrelations within the confidence bands. For the Ljung - Box Test, the Null hypothesis (H 0 ) is that the first 20 auto correlation coefficients are jointly equal to 0 for the model residuals. The a lternate hypothesis (H a ) is the complement of the null. > Box.test(arma22$residuals,lag=20, type = \"Ljung - Box\") Box - Ljung test data: arma22$residuals X - squared = 32.171, df = 20, p - value = 0.04151 This P - value means that 4.151% of all 2 random variables are less than test statistic, and we need this to be below 5%, so we will fail to reject the null hypothesis. Out of the models investigated, the ARMA(2,2) model was the only model that failed to reject the null hypothesis with the Ljung - Box Test, therefore I will continue to the forecast with this model. PROBLEM SET 3 ECON 5337 ALEXANDER WILSON E. The 7 - step ahead forecast from the AR(2,2) model and two graphs to show the forecast in relation to the prior time periods are below: > forecast.Arima(arma22, h=7) Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 2015 Q2 11.78419 11.130794 12.43758 10.784909 12.78346 2015 Q3 11.58376 10.475082 12.69244 9.888182 13.27934 2015 Q4 11.51714 9.951859 13.08242 9.123249 13.91103 2016 Q1 11.51 985 9.577697 13.46200 8.549584 14.49011 2016 Q2 11.55652 9.318960 13.79408 8.134466 14.97858 2016 Q3 11.60797 9.143174 14.07277 7.838388 15.37756 2016 Q4 11.66399 9.025613 14.30236 7.628944 15.69903 > plot.forecast(forecast.Arima (arma22, h=7), include=250) (This plot was intentionally scaled up to show a long time series) PROBLEM SET 3 ECON 5337 ALEXANDER WILSON > plot.forecast(forecast.Arima(arma22, h=7), include=20) These plots show the predicted values, and the increasing uncertainty with the forecasts at the further steps ahead.', 'Domain': None, 'Hash': '0ce8157f849fe4c22bbc154ec55b68b9c95e7152', 'File_Size': 355708L, 'URL': None, 'File_Name': 'ajs ps3.pdf', 'Content_Hash': '518198b5472e7e3ef1dc8dcbc9d55df121a9f4ac', 'Pages': 10, 'Folder': None, 'Metadata': {'/CreationDate': u\"D:20161122123023-06'00'\", '/ModDate': u\"D:20161122123023-06'00'\", '/Producer': u'Microsoft\\xae Word 2016', '/Creator': u'Microsoft\\xae Word 2016', '/Author': u'Alexander Wilson'}}\n",
      "A duplicate hash was not found in the database.\n",
      "There is no match for this file in the database\n"
     ]
    }
   ],
   "source": [
    "user_input = {}\n",
    "user_file = raw_input(\"Enter File Path or URL: \").lower()\n",
    "\n",
    "if user_file[-4:] == '.pdf':\n",
    "    if user_file[:4] == 'http':\n",
    "        user_input.update({'URL':user_file})\n",
    "        user_input.update({'File_Location':\"web\"})\n",
    "    elif user_file[1:3] == ':\\\\':\n",
    "        user_input.update({'Local_Path':user_file})\n",
    "        user_input.update({'URL':None})\n",
    "        user_input.update({'File_Location':\"local\"})\n",
    "    else: print(\"File Type Error: Please enter a URL or full local path\")\n",
    "else: print('Only .pdf files are supported at this time')\n",
    "    \n",
    "if user_input['File_Location']=='web':\n",
    "    existing_file = collection.find_one({u\"URL\":(user_input['URL']).lower()})\n",
    "    if existing_file != None:\n",
    "        print('The file is already in the database as UID: {}'.format(existing_file['UID']))\n",
    "        print(existing_file)\n",
    "    else:\n",
    "        import hashlib\n",
    "        file_binary=download_from_url(user_input['URL'])\n",
    "        file_hash = hashlib.sha1(file_binary).hexdigest()\n",
    "        print(file_hash)\n",
    "        existing_file = collection.find_one({u\"Hash\":file_hash})\n",
    "        \n",
    "        import PyPDF2\n",
    "        from StringIO import StringIO\n",
    "        pdf_in_memory = PyPDF2.PdfFileReader(StringIO(file_binary))\n",
    "        \n",
    "        if existing_file != None:\n",
    "            print('The file is already in the database as UID: {}'.format(existing_file['UID']))\n",
    "            print(existing_file)\n",
    "        else:\n",
    "            print('A duplicate hash was not found in the database.')\n",
    "            \n",
    "            url_parts = user_input[\"URL\"].strip().split('/') \n",
    "            user_input.update({'Domain':url_parts[2]})\n",
    "            user_input.update({'Folder':url_parts[-2]})\n",
    "            user_input.update({'File_Name':url_parts[-1]})\n",
    "            \n",
    "            user_input.update({'Hash':file_hash})\n",
    "\n",
    "            content_extract = extract_text_content_and_pages(pdf_in_memory)\n",
    "            user_input.update({'Pages':content_extract[1]})\n",
    "            user_input.update({'Content':content_extract[0]})\n",
    "            \n",
    "            new_content_hash = hashlib.sha1(content_extract[0]).hexdigest()\n",
    "            user_input.update({'Content_Hash': new_content_hash})\n",
    "            \n",
    "            user_input.update({'Metadata':pdf_in_memory.documentInfo})\n",
    "            \n",
    "            import pickle\n",
    "            current_UID_position = pickle.load(open('current_uid.pickle','r'))\n",
    "            user_input.update({'UID':'BSA_{}'.format(current_UID_position)})\n",
    "            current_UID_position +=1\n",
    "            with open('current_uid.pickle','w') as p:\n",
    "                pickle.dump(current_UID_position, p)\n",
    "            \n",
    "            import sys\n",
    "            user_input.update({'File_Size':sys.getsizeof(file_binary)})\n",
    "            \n",
    "            existing_content_hash_doc = collection.find_one({\"Content_Hash\":new_content_hash})\n",
    "            \n",
    "            collection.insert_one(user_input)\n",
    "            \n",
    "            if existing_content_hash_doc != None:\n",
    "                print('A file with matching content was found as UID: {}'.format(existing_content_hash_doc['UID']))\n",
    "                print(existing_content_hash_doc)\n",
    "            else:\n",
    "                print('This is a file with new content and has been loaded as BSA_{}'.format(current_UID_position-1))\n",
    "                print('*'*75)\n",
    "                print(user_input)\n",
    "else:\n",
    "    local_file_dict = {} #blank dictionary to add data to\n",
    "    local_pdf = PyPDF2.PdfFileReader(file(user_input['Local_Path'],'rb'))\n",
    "    \n",
    "    local_file_dict.update({'Metadata':local_pdf.documentInfo})\n",
    "    \n",
    "    import os\n",
    "    local_file_dict.update({'File_Size': os.path.getsize(user_input['Local_Path'])})\n",
    "    \n",
    "    local_file_hash = hashlib.sha1(open(user_input['Local_Path'],'rb').read()).hexdigest() \n",
    "    local_file_dict.update({'Hash':local_file_hash})\n",
    "    \n",
    "    path_parts = user_input['Local_Path'].strip().split('\\\\')\n",
    "    local_file_dict.update({'File_Name':path_parts[-1]})\n",
    "    \n",
    "    content_extract = extract_text_content_and_pages(local_pdf)\n",
    "    local_file_dict.update({'Pages':content_extract[1]})\n",
    "    local_file_dict.update({'Content':content_extract[0]})\n",
    "    local_file_dict.update({'Content_Hash': hashlib.sha1(content_extract[0]).hexdigest()})\n",
    "    \n",
    "    local_file_dict.update({'URL':None})\n",
    "    local_file_dict.update({'Domain':None}) \n",
    "    local_file_dict.update({'Folder':None}) \n",
    "    \n",
    "    \n",
    "    #now we need to check for duplicates!\n",
    "    \n",
    "    bin_hash_match = collection.find_one({u\"Hash\":local_file_dict['Hash']})\n",
    "    \n",
    "    if bin_hash_match != None:\n",
    "        print('The file is already in the database as UID: {}'.format(bin_hash_match['UID']))\n",
    "        print(bin_hash_match)\n",
    "    else:\n",
    "        print('A duplicate hash was not found in the database.')\n",
    "        \n",
    "        content_hash_match = collection.find_one({\"Content_Hash\":local_file_dict['Content_Hash']})\n",
    "        \n",
    "        if content_hash_match != None:\n",
    "            print('The file content already in the database as UID: {}'.format(content_hash_match['UID']))\n",
    "            print(content_hash_match)\n",
    "        else:\n",
    "            print('There is no match for this file in the database. Please re-submit as a URL once the file is loaded to the web')\n",
    "    print('*'*75)\n",
    "    print(local_file_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.cbu.edu.zm/downloads/pdf-sample.pdf\n",
    "# http://tdc-www.harvard.edu/Python.pdf\n",
    "# http://www.ucs.cam.ac.uk/docs/course-notes/unix-courses/PythonAB/files/handout.pdf\n",
    "# http://adventure.oa-bsa.org/files/flyers/OAHA_5_program_flyer.pdf\n",
    "# http://www.scouting.org/filestore/boyscouts/pdf/Troop_Leadership_Positions.pdf\n",
    "#\n",
    "#\n",
    "#D:\\BSA_PDF_Files\\BSA_107.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
