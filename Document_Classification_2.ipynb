{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared by: AJ Wilson\n",
      "Last Edit: 2017-02-15 15:19:51.808000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python 2.7.12 :: Anaconda custom (64-bit)\n"
     ]
    }
   ],
   "source": [
    "author = 'AJ Wilson'\n",
    "from datetime import datetime\n",
    "print(\"Prepared by: {}\".format(author))\n",
    "print(\"Last Edit: {}\".format(datetime.now()))\n",
    "!python -V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# **Document Clustering and Matching**\n",
    "## Unsupervised Learning Test 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import re\n",
    "import string\n",
    "\n",
    "#sklearn\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MaxAbsScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, NMF, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#scipy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "#matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#location in other project folder where the source documents are\n",
    "file_path = 'C:\\\\Users\\\\ajwil\\\\OneDrive\\\\Documents\\\\00_public\\\\Group Projects\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mongod.exe --dbpath D:\\mongodata\n",
    "from pymongo import MongoClient\n",
    "\n",
    "connection = MongoClient()\n",
    "db = connection.bsa_files\n",
    "collection = db.bsa_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4221, list)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_content_hash_list = pickle.load(open((file_path+'unique_content_hash_list.pickle'),'r'))\n",
    "len(unique_content_hash_list),type(unique_content_hash_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_text_corpus = []\n",
    "document_names = []\n",
    "used_content_hashes = []\n",
    "discarded_content_hashes = []\n",
    "\n",
    "for item in unique_content_hash_list:\n",
    "    document = collection.find_one({'Content_Hash':item})\n",
    "    \n",
    "    if document['Content'] != None and len(document['Content'])>1 and len(document['Content'])<500000 and document['Language']=='english':\n",
    "        full_text_corpus.append(document['Content'])\n",
    "        document_names.append(document['File_Name'])\n",
    "        used_content_hashes.append(item)\n",
    "    else:\n",
    "        discarded_content_hashes.append(item)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(discarded_content_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3961, 3961)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_text_corpus), len(document_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extra_stop_words = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "my_stopwords = stopwords.words('english') + stopwords.words('spanish') + extra_stop_words\n",
    "\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'[0-9_]','',text)\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token) and token not in my_stopwords and len(token)<30: #30 = len(longest english word)\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csr_mat = tfidf.fit_transform(full_text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3961L, 191419L)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_mat.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler = MaxAbsScaler()\n",
    "nmf = NMF(n_components = 25)\n",
    "normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(scaler, nmf, normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_features = pipeline.fit_transform(csr_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3961L, 25L)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(norm_features, index = document_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document = df.loc[document_names[50]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accounts_Payable_Year_End_2015.pdf        1.000000\n",
       "Year_End_Accounts%20Payable_2013.pdf      0.998363\n",
       "GL_Summary_Loading_PR_%208-3-11.pdf       0.989468\n",
       "Record_Product_Sales.pdf                  0.985634\n",
       "fm_pro_for_ste.pdf                        0.983687\n",
       "JTE_Glossary.pdf                          0.979944\n",
       "Gift-Policies-Procedures-BSAF-2014.pdf    0.979682\n",
       "PeopleSoft_User_Group_20150909.pdf        0.979543\n",
       "Chapter_13_Asset_Management.pdf           0.979366\n",
       "bp-newsletter-gift-chart.pdf              0.978068\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = df.dot(document)\n",
    "similarities.nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Camping.pdf                   1.000000\n",
       "Camping_previous.pdf          0.999717\n",
       "Backpacking_previous.pdf      0.998194\n",
       "Backpacking.pdf               0.998068\n",
       "Fly-Fishing.pdf               0.989535\n",
       "Woodwork.pdf                  0.985105\n",
       "Automotive_Maintenance.pdf    0.982050\n",
       "Fire_Safety.pdf               0.980162\n",
       "Crime_Prevention.pdf          0.980143\n",
       "First_Aid.pdf                 0.979337\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document = df.loc[document_names[15]]\n",
    "similarities = df.dot(document)\n",
    "similarities.nlargest(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#temp1 = pickle.dumps(csr_mat)\n",
    "#temp2 = pickle.loads(temp1)\n",
    "#temp2.toarray() == csr_mat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#temp1 = pickle.dumps(df)\n",
    "#temp2 = pickle.loads(temp1)\n",
    "#temp2 == df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x1d325828>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.update_one({\"Document_Class\":\"english_resources\"},{\"$set\":{\"TFIDF_Vocabulary\":words}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191419"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collection.find_one({\"Document_Class\":\"english_resources\"})[\"TFIDF_Vocabulary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf2 = TfidfVectorizer(tokenizer=tokenize, vocabulary = collection.find_one({\"Document_Class\":\"english_resources\"})[\"TFIDF_Vocabulary\"])\n",
    "test1 = tfidf.transform(full_text_corpus[0:2])\n",
    "test2 = tfidf2.fit_transform(full_text_corpus[0:2])\n",
    "test1.toarray() == test2.toarray()\n",
    "#test1.shape, test2.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
