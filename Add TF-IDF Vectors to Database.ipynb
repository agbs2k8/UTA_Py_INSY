{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to do: \n",
    "1. Preserve TF-IDF Vectorizer for re-use (*done- can vectorize new document with same vocab*)\n",
    "2. Create matrix of document distances \n",
    "    (*exists as document distances.  Needs to be able to expand to add new documents, or at least just measure new against existing \n",
    "        Content Hash is added to resource objects in database\n",
    "        Distance Matrix cannot be loaded to DB as is, working to pickle to StringIO, store and retrieve from DB as binary object*)\n",
    "3. Find thresholds for document matching\n",
    "\n",
    "Other ways to compare documents\n",
    "1. Word count\n",
    "2. Iterations per word\n",
    "3. Compare sentances: if sentance in other document & vice/versa\n",
    "\n",
    "Concept to test: \n",
    "    If documents are very similar by TF-IDF vector, compare word-count and assume larger is the base document.  Compare iterations of every word between the two documents... what % of smaller doc's words are in the larger doc vs what % of larger doc's words that are in the smaller.  Last, cut smaller document into sentances, and see how many (%) of the sentances are found in the larger document.  \n",
    "    \n",
    "Older documents might be smaller, that they would grow over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n",
    "logging.root.level = logging.INFO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import pickle\n",
    "from StringIO import StringIO\n",
    "\n",
    "#full packages\n",
    "#import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#import os\n",
    "#import codecs\n",
    "\n",
    "#sklearn tools\n",
    "#from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances \n",
    "#from sklearn.manifold import MDS\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.externals import joblib\n",
    "\n",
    "#nltk tools\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "#scipy tools\n",
    "#from scipy.cluster.hierarchy import ward, dendrogram\n",
    "\n",
    "#gensim\n",
    "#from gensim import corpora, models, similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from django.utils.encoding import smart_str, smart_unicode\n",
    "def convertU(data):\n",
    "    if isinstance(data, basestring):\n",
    "        return smart_str(data)\n",
    "    elif isinstance(data, collections.Mapping):\n",
    "        return dict(map(convertU, data.iteritems()))\n",
    "    elif isinstance(data, collections.Iterable):\n",
    "        return type(data)(map(convertU, data))\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "# Connect to the mongo local database\n",
    "connection = MongoClient()\n",
    "db = connection.bsa_files\n",
    "collection = db.bsa_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_BaseObject__read_preference',\n",
       " '_Collection__create',\n",
       " '_Collection__create_index',\n",
       " '_Collection__database',\n",
       " '_Collection__find_and_modify',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__iter__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__next__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '_command',\n",
       " '_count',\n",
       " '_delete',\n",
       " '_insert',\n",
       " '_insert_one',\n",
       " '_legacy_write',\n",
       " '_socket_for_primary_reads',\n",
       " '_socket_for_reads',\n",
       " '_socket_for_writes',\n",
       " '_update',\n",
       " 'aggregate',\n",
       " 'bulk_write',\n",
       " 'count',\n",
       " 'create_index',\n",
       " 'create_indexes',\n",
       " 'database',\n",
       " 'delete_many',\n",
       " 'delete_one',\n",
       " 'distinct',\n",
       " 'drop',\n",
       " 'drop_index',\n",
       " 'drop_indexes',\n",
       " 'ensure_index',\n",
       " 'find',\n",
       " 'find_and_modify',\n",
       " 'find_one',\n",
       " 'find_one_and_delete',\n",
       " 'find_one_and_replace',\n",
       " 'find_one_and_update',\n",
       " 'group',\n",
       " 'index_information',\n",
       " 'initialize_ordered_bulk_op',\n",
       " 'initialize_unordered_bulk_op',\n",
       " 'inline_map_reduce',\n",
       " 'insert',\n",
       " 'insert_many',\n",
       " 'insert_one',\n",
       " 'list_indexes',\n",
       " 'map_reduce',\n",
       " 'next',\n",
       " 'options',\n",
       " 'parallel_scan',\n",
       " 'read_preference',\n",
       " 'reindex',\n",
       " 'remove',\n",
       " 'rename',\n",
       " 'replace_one',\n",
       " 'save',\n",
       " 'update',\n",
       " 'update_many',\n",
       " 'update_one',\n",
       " 'with_options']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! python -c \"import pymongo;print pymongo.__version__\"\n",
    "[method for method in dir(collection) if callable(getattr(collection, method))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Text Corpus and perform Corpus wide TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4221\n"
     ]
    }
   ],
   "source": [
    "unique_content_hash_list = pickle.load(open('unique_content_hash_list.pickle','r'))\n",
    "print(len(unique_content_hash_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_text_corpus = []\n",
    "english_content_hash = []\n",
    "spanish_text_corpus = []\n",
    "spanish_content_hash = []\n",
    "\n",
    "for unique_hash in unique_content_hash_list:\n",
    "    document = collection.find_one({\"Content_Hash\":unique_hash})\n",
    "    if document[\"Content\"]!=None and len(document[\"Content\"])>1:\n",
    "        if document[\"Language\"]=='english':\n",
    "            english_text_corpus.append(document[\"Content\"])\n",
    "            english_content_hash.append(document[\"Content_Hash\"])\n",
    "        elif document[\"Language\"]=='spanish':\n",
    "            spanish_text_corpus.append(document[\"Content\"])\n",
    "            spanish_content_hash.append(document[\"Content_Hash\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3979 3979\n",
      "240 240\n"
     ]
    }
   ],
   "source": [
    "print(len(english_text_corpus),len(english_content_hash))\n",
    "print(len(spanish_text_corpus),len(spanish_content_hash))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#special stopwords to remove common scouting terms and other erroneous text that continually showed up in the results\n",
    "bsa_stop_words = ['scout','boy','cub','train','council',\n",
    "                  'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']\n",
    "\n",
    "english_stopwords = stopwords.words('english') + bsa_stop_words\n",
    "spanish_stopwords = stopwords.words('spanish') + bsa_stop_words + stopwords.words('english')\n",
    "\n",
    "english_stemmer = SnowballStemmer(\"english\")\n",
    "spanish_stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "def tokenize_and_stem_english(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [english_stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_and_stem_spanish(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [spanish_stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in sent_tokenize(text) for word in word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_vocab_stemmed = []\n",
    "english_vocab_tokenized = []\n",
    "spanish_vocab_stemmed = []\n",
    "spanish_vocab_tokenized = []\n",
    "\n",
    "for e in english_text_corpus:\n",
    "    words_stemmed = tokenize_and_stem_english(e)\n",
    "    english_vocab_stemmed.extend(words_stemmed)\n",
    "    words_tokenized = tokenize_only(e)\n",
    "    english_vocab_tokenized.extend(words_tokenized)\n",
    "\n",
    "for s in spanish_text_corpus:\n",
    "    words_stemmed = tokenize_and_stem_spanish(s)\n",
    "    spanish_vocab_stemmed.extend(words_stemmed)\n",
    "    words_tokenized = tokenize_only(s)\n",
    "    spanish_vocab_tokenized.extend(words_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10947405 10947405\n",
      "1986384 10947405\n"
     ]
    }
   ],
   "source": [
    "print(len(english_vocab_stemmed),len(english_vocab_tokenized))\n",
    "print(len(spanish_vocab_stemmed),len(english_vocab_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10947405 items in english_vocab_frame\n",
      "There are 1986384 items in spanish_vocab_frame\n"
     ]
    }
   ],
   "source": [
    "english_vocab_frame = pd.DataFrame({'words':english_vocab_tokenized}, index = english_vocab_stemmed)\n",
    "print('There are {} items in english_vocab_frame'.format(str(english_vocab_frame.shape[0])))\n",
    "\n",
    "spanish_vocab_frame = pd.DataFrame({'words':spanish_vocab_tokenized}, index = spanish_vocab_stemmed)\n",
    "print('There are {} items in spanish_vocab_frame'.format(str(spanish_vocab_frame.shape[0])))\n",
    "\n",
    "with open('english_vocab_frame2.pickle','w') as p:\n",
    "    pickle.dump(english_vocab_frame,p)\n",
    "with open('spanish_vocab_frame2.pickle','w') as q:\n",
    "    pickle.dump(spanish_vocab_frame,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_vocab_frame = pickle.load(open('english_vocab_frame2.pickle','r'))\n",
    "spanish_vocab_frame = pickle.load(open('spanish_vocab_frame2.pickle','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_tfidf_vectorizer = TfidfVectorizer(max_df=0.8, \n",
    "                                           max_features=200000,\n",
    "                                           min_df=0.2, \n",
    "                                           stop_words=english_stopwords,\n",
    "                                           use_idf=True, \n",
    "                                           tokenizer=tokenize_and_stem_english, \n",
    "                                           ngram_range=(1,3))\n",
    "\n",
    "spanish_tfidf_vectorizer = TfidfVectorizer(max_df=0.8, \n",
    "                                           max_features=200000,\n",
    "                                           min_df=0.2, \n",
    "                                           stop_words=spanish_stopwords,\n",
    "                                           use_idf=True, \n",
    "                                           tokenizer=tokenize_and_stem_spanish, \n",
    "                                           ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3979, 313)\n",
      "(240, 790)\n"
     ]
    }
   ],
   "source": [
    "english_tfidf_matrix = english_tfidf_vectorizer.fit_transform(english_text_corpus)\n",
    "print(english_tfidf_matrix.shape)\n",
    "\n",
    "spanish_tfidf_matrix = spanish_tfidf_vectorizer.fit_transform(spanish_text_corpus)\n",
    "print(spanish_tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_terms = english_tfidf_vectorizer.get_feature_names()\n",
    "spanish_terms = spanish_tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313 790\n"
     ]
    }
   ],
   "source": [
    "print(len(english_terms),len(spanish_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "english_dist = 1-cosine_similarity(english_tfidf_matrix)\n",
    "spanish_dist = 1-cosine_similarity(spanish_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('english_distance2.pickle','w') as p:\n",
    "    pickle.dump(english_dist,p)\n",
    "with open('spanish_distance2.pickle','w') as q:\n",
    "    pickle.dump(spanish_dist,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3979L, 3979L)\n"
     ]
    }
   ],
   "source": [
    "print(english_dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n",
      "[3, 6, 21, 33, 39, 52, 99, 127, 131, 137, 164, 170, 189, 192, 202, 208, 227, 242, 259, 273, 279, 280, 281, 301, 341, 349, 362, 363, 387, 392, 394, 409, 416, 455, 482, 486, 501, 514, 517, 528, 567, 569, 576, 581, 585, 627, 628, 644, 670, 675, 676, 681, 690, 704, 720, 722, 738, 804, 808, 815, 818, 820, 828, 832, 853, 871, 891, 926, 944, 946, 955, 977, 987, 989, 1007, 1015, 1021, 1025, 1028, 1031, 1078, 1093, 1110, 1125, 1133, 1149, 1155, 1162, 1175, 1176, 1180, 1185, 1193, 1215, 1226, 1240, 1264, 1266, 1277, 1305, 1307, 1320, 1344, 1347, 1349, 1361, 1362, 1366, 1380, 1392, 1411, 1412, 1419, 1424, 1430, 1440, 1441, 1442, 1463, 1477, 1485, 1495, 1502, 1529, 1557, 1565, 1583, 1584, 1596, 1613, 1615, 1616, 1629, 1641, 1643, 1673, 1705, 1709, 1722, 1746, 1757, 1758, 1807, 1815, 1841, 1852, 1861, 1896, 1897, 1924, 1935, 1936, 1941, 1944, 1955, 1974, 1976, 1999, 2057, 2080, 2082, 2095, 2128, 2151, 2169, 2171, 2175, 2211, 2215, 2264, 2283, 2302, 2306, 2372, 2373, 2395, 2417, 2434, 2441, 2444, 2461, 2471, 2486, 2507, 2515, 2529, 2606, 2620, 2633, 2635, 2637, 2660, 2673, 2722, 2726, 2727, 2736, 2749, 2788, 2789, 2791, 2840, 2888, 2907, 2908, 2923, 2924, 2927, 2932, 2961, 2964, 2982, 2985, 2987, 3001, 3002, 3036, 3042, 3044, 3045, 3066, 3092, 3098, 3101, 3102, 3112, 3122, 3137, 3149, 3152, 3178, 3193, 3221, 3233, 3236, 3239, 3240, 3270, 3290, 3314, 3323, 3330, 3357, 3417, 3418, 3420, 3442, 3504, 3505, 3525, 3533, 3548, 3566, 3579, 3625, 3642, 3661, 3687, 3697, 3698, 3727, 3738, 3739, 3748, 3792, 3802, 3809, 3815, 3834, 3849, 3860, 3873, 3888, 3918, 3932]\n"
     ]
    }
   ],
   "source": [
    "non_equals = []\n",
    "for x in range (0,3979):\n",
    "    if abs(english_dist[x][x]) > 0.00001:\n",
    "        non_equals.append(x)\n",
    "print(len(non_equals))\n",
    "print(non_equals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "˘ˇˆ˙˘˝˛ ˝˚˜ !\"!# $% ˝&˝''˝ (˙˝( ˙˚˝% ) *)&+ )˙ ,˝ '˝'˜ ! - ˜\" '˝% ˙˚ ˘˚ ! ˙˚ ˘''˚ ( ./012 ,˝˚˚ \" .* - ˚ ))) 3-' , \" /˚2% 4 +'-5&˚ 4-˝',,˜ ! ˝',˘\" ˆ˝,˘ ! ˘!˚˝#+6 57 ,˚˝,˘\" ˝,˘ 5 .˚'˝˚'\"˝ ˝ - ,' 80 *˝ ˘˘ˇ˘ˇ +˝/(˙ 2\" - ˘˚' /$%# ˝2 ˘!˚˝#+6 59 /$%# ˝2 .˝\" '˝' (\"˝˚ ''' ' ˚\"˝ ˚ /$%'˝ '˝\" ˜ !-!2 $! $! $! ˘!˚˝#+6 51 ˆˇ˙˝ˇ .&˝˚\" ˚* '#˚ ˝* ˘'˚ ˚˚˛ ˝'\"˚ ˝˝˘ .&\" '˜˚ !#˝˝ 7:87 #˝ ;ˆ<˝''/ '2.˝ ˝\"˝˝ #˝\"˝ \"˛˝\"˜ !#\"= -\"=(/ ˝ !#\"=!#\"!#2 ˘!˚˝#+6 50 ˛\" ; ˚<˝'' , ˚\" ;<˝/>˙? 2 ˝ & ˝ ;˜<;@<˝˝˚ ˚'* # ' ˛ˇ˘ˇ \"'\" \"˝' ''\"˝ *˝ ˝' ˚˝ ˙ ', \"˝ !#'\n"
     ]
    }
   ],
   "source": [
    "x = 3860\n",
    "print(english_dist[0][x])\n",
    "print(english_dist[x][0])\n",
    "print(english_dist[x][x])\n",
    "print(english_text_corpus[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(english_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'> <type 'list'> <type 'numpy.ndarray'> <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(english_content_hash), \n",
    "      type(english_text_corpus), \n",
    "      type(english_dist),\n",
    "      type(english_dist[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c7ba405cc29859106ac28a01c71ad03c5393644d\n",
      "***************************************************************************\n",
      "<type 'unicode'>\n",
      "Solid to the Core Providing Resources to Support Units, Chapters, and Lodges Every once in a while when you™re working on a pioneer ing project, you™ll find a spar that looks great but that turns out to be weak and unreliable. Maybe its center has been eaten away by insects. Or maybe it has natural splits inside that you can't see. You can test a spar fo r soundness by holding one end and rapping the other end sharply on a rock. If it™s sound you'll hear it ring. Otherwise, you™ll want to toss it aside and find a good, solid spar to work with. Some people are like defective spars. They look great on the outside and they may have appealing personalities, the kind of guys and girls you think you would like to know. But when you do get to know them better, you find that they™re like a defective sparŠweak inside . They don™t have the strength of character to resist things that you know are wrong, and chances are they will want you to do those things, too. When that happens, do the same thing you do when you have a defective spar: Cast it aside and find a sounder one. In other words, choose friends who are solid to the core.\n",
      "***************************************************************************\n",
      "3979\n",
      "[ -2.22044605e-16   9.41822331e-01   7.85093188e-01 ...,   7.97834947e-01\n",
      "   8.56313364e-01   9.17019276e-01]\n",
      "***************************************************************************\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 109)\t0.115673811426\n",
      "  (0, 278)\t0.353851135641\n",
      "  (0, 42)\t0.11552896921\n",
      "  (0, 114)\t0.0985952764511\n",
      "  (0, 145)\t0.311407315168\n",
      "  (0, 309)\t0.103124969061\n",
      "  (0, 204)\t0.0946076627929\n",
      "  (0, 167)\t0.0822299032742\n",
      "  (0, 155)\t0.299616186464\n",
      "  (0, 201)\t0.101724951378\n",
      "  (0, 118)\t0.101219126841\n",
      "  (0, 299)\t0.22345593521\n",
      "  (0, 91)\t0.225894159558\n",
      "  (0, 188)\t0.159615735699\n",
      "  (0, 252)\t0.100258188097\n",
      "  (0, 51)\t0.104221131372\n",
      "  (0, 285)\t0.125853570385\n",
      "  (0, 119)\t0.216653334398\n",
      "  (0, 161)\t0.229717345236\n",
      "  (0, 102)\t0.493426187263\n",
      "  (0, 220)\t0.104412641912\n",
      "  (0, 307)\t0.174719397251\n",
      "  (0, 187)\t0.12028533448\n",
      "  (0, 95)\t0.105069464603\n",
      "  (0, 289)\t0.0840403245777\n",
      "  (0, 272)\t0.0949225176974\n",
      "  (0, 240)\t0.0927298798442\n",
      "  (0, 223)\t0.0817213139319\n"
     ]
    }
   ],
   "source": [
    "print(english_content_hash[0])\n",
    "print('*'*75)\n",
    "print(type(english_text_corpus[0]))\n",
    "print(english_text_corpus[0])\n",
    "print('*'*75)\n",
    "print(len(english_dist[0]))\n",
    "print(english_dist[0])\n",
    "print('*'*75)\n",
    "print(type(english_tfidf_matrix[0]))\n",
    "print(english_tfidf_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Append TF-IDF array to DB Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(1L, 313L)\n",
      "313\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.11552897  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.10422113  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.22589416  0.          0.          0.          0.10506946  0.          0.\n",
      "  0.          0.          0.          0.          0.49342619  0.          0.\n",
      "  0.          0.          0.          0.          0.11567381  0.          0.\n",
      "  0.          0.          0.09859528  0.          0.          0.\n",
      "  0.10121913  0.21665333  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.31140732  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.29961619  0.          0.          0.\n",
      "  0.          0.          0.22971735  0.          0.          0.          0.\n",
      "  0.          0.0822299   0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.12028533  0.15961574  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.10172495  0.          0.          0.09460766  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.10441264\n",
      "  0.          0.          0.08172131  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.09272988\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.10025819  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.09492252  0.          0.          0.\n",
      "  0.          0.          0.35385114  0.          0.          0.          0.\n",
      "  0.          0.          0.12585357  0.          0.          0.\n",
      "  0.08404032  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.22345594  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.1747194   0.\n",
      "  0.10312497  0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "temp_array = english_tfidf_matrix[0].toarray()\n",
    "print(type(temp_array))\n",
    "print(temp_array.shape)\n",
    "print(len(temp_array[0]))\n",
    "print(temp_array[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "313\n"
     ]
    }
   ],
   "source": [
    "simple_array = english_tfidf_matrix[0].toarray()[0].tolist()\n",
    "print(type(simple_array))\n",
    "print(len(simple_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x16d738b40>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.update_many({\"Content_Hash\":english_content_hash[0]},\n",
    "                       {'$set':{\"TFIDF_Vector\":english_tfidf_matrix[0].toarray()[0].tolist()}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for x in range(0,len(english_content_hash)):\n",
    "    collection.update_many({\"Content_Hash\":english_content_hash[x]},\n",
    "                       {'$set':{\"TFIDF_Vector\":english_tfidf_matrix[x].toarray()[0].tolist()}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Domain': u'bsaseabase.org', u'Hash': u'acd7f9d3865638176146d2eaf8698313fdfc83b3', u'UID': u'BSA_631', u'Language': u'english', u'URL': u'http://bsaseabase.org/filestore/commissioner/pdf/commissioner_update_march2016.pdf', u'File_Name': u'Commissioner_Update_March2016.pdf', u'Content': u'Commissioner Update March 2016 Last week we announced a new report called In Progress Contacts that can be run from the Report tab on your district and council dashboards. It is located after the District Contact Stats 2016 report in the drop down list. The reason we added this report was so commissioners can better manage those contacts that have not been marked complete. The report can be us ed by administrative commissioners to insure work recorded is not lost. Once a contact is started a commissioner or professional has 60 days to complete it. This limitation was set to encourage timely action be taken which will in turn allow for administ rative commissioners and district executives the ability to get the unit the appropriate assistance they need. If a contact appears on the report as In Progress the commissioner who created it should go to the unit dashboard where he can locate the contac t and finish it by clicking on the contact. This window will open. If you want to add more information click the appropriate simple or detailed assessment button, complete the scoring and clic k the save contact button. It is suggested that before allowing those contacts started in January or February and left in Progress to expire, that commissioners take the time to complete them as soon as possible. This will allow them to be credited to JTE trac king as well. ALSO with the changes last week to split out the Unit Service Plan into a separate step (step 7) on the detailed assessment y ou can now mark a contact with a Unit Service Plan as complete and still come back later and update Step 7 in the Uni t Service plan without the 60 day limitation . Thus the benefit of moving the Unit Service plan to its own step permits commissioners more time to track and assist their units who are working on improvements.', u'Content_Hash': u'7f56c9156ce7ee54178c59859e5aa208eb532be6', u'File_Size': 193093, u'TFIDF_Vector': [0.0, 0.0, 0.0, 0.0, 0.06688393951447606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1810150193135638, 0.0, 0.048590886511843276, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1274619490254728, 0.0, 0.0, 0.0, 0.0, 0.11186231049832206, 0.0, 0.0, 0.0, 0.0, 0.06322666610086396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05629388488152418, 0.0, 0.0, 0.06338489629354542, 0.0, 0.0, 0.0, 0.0, 0.0, 0.058824303861086474, 0.0, 0.0, 0.0, 0.0, 0.05873524191517786, 0.0, 0.0, 0.0, 0.06110446771829182, 0.0, 0.0, 0.0, 0.2312382855427134, 0.0, 0.0, 0.4999818413272308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05820701092102742, 0.0, 0.0, 0.0, 0.10955113632270336, 0.0, 0.13523489623848278, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18094424518138577, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.061644209105914706, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06067082322278009, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05409423641187305, 0.0, 0.05477556816135168, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06330566416286776, 0.0, 0.0, 0.0, 0.04662656781069405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12408741195668727, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05545504490583215, 0.0, 0.0, 0.12383644176793489, 0.0, 0.0, 0.0, 0.0, 0.06057540537117235, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046739876558788994, 0.04620588543974109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06599447310703598, 0.0, 0.0, 0.06244926180164405, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19243405430216956, 0.0, 0.0, 0.0, 0.06250039150085451, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06530360606011426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.061669003567162754, 0.0, 0.0, 0.0, 0.36691852447600726, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06944467402579957, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.18404723655033386, 0.056620494432309376, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.11222381006598313, 0.0, 0.2660983796777951, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.05147939232218604, 0.0, 0.0, 0.0, 0.0, 0.1338770551617448, 0.0, 0.0, 0.0, 0.0, 0.0690494822341243, 0.0, 0.0, 0.0, 0.2766520273267663, 0.13648891967088272, 0.0, 0.06405445617362884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.06129947915349759, 0.0, 0.0, 0.1264007703802191, 0.05621285201787853, 0.0, 0.0, 0.06830797726326307, 0.09585968740913448, 0.0, 0.0, 0.0, 0.0, 0.0], u'Folder': u'pdf', u'_id': ObjectId('582cc15c8726e30d949a9a1f'), u'Pages': 2, u'Metadata': {u'/ModDate': u\"D:20160305141756-06'00'\", u'/Creator': u'Microsoft\\xae Word 2013', u'/Producer': u'Microsoft\\xae Word 2013', u'/CreationDate': u\"D:20160305141756-06'00'\", u'/Author': u'Debra Kendrew'}}\n"
     ]
    }
   ],
   "source": [
    "print(collection.find_one({\"Content_Hash\":english_content_hash[-1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in range(0,len(spanish_content_hash)):\n",
    "    collection.update_many({\"Content_Hash\":spanish_content_hash[x]},\n",
    "                       {'$set':{\"TFIDF_Vector\":spanish_tfidf_matrix[x].toarray()[0].tolist()}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Domain': u'bsaseabase.org', u'Hash': u'8fa8d904b9ba9f0326473ad594b85cc44af96e62', u'UID': u'BSA_1806', u'Language': u'spanish', u'URL': u'http://bsaseabase.org/filestore/marketing/310-740_spn/venturing/310-740-52_fliers/pdfs/310-740-52-3_fill.pdf', u'File_Name': u'310-740-52-3_fill.pdf', u'Folder': u'PDFs', u'Content': u'Toma sel\\u02dces. Sirve al pr\\xf3jimo.Obt\\xe9n con\\u02dcanza.Construye amistades.Toma riesgos. Convive.Haz la diferencia.Busca oportunidades.Deja un legado.Orienta a otros.Persigue la aventura.Ampl\\xeda tus horizontes.AQU\\xcd COMIENZA LA AVENTURA EN SEUNSCOUT.ORG. LA BASE PARA UN GRAN FUTURO.', u'Document_Class': u'Marketing Experience Mgmt', u'File_Size': 4217834, u'TFIDF_Vector': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.28449080949378985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26344992298426273, 0.36372622791973963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.24924637542590597, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33107781358537286, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27363879415387454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.40726095895944475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3386605239075383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34660444909907506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.26469000974319873, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], u'Content_Hash': u'5dd4cd1c8f3e0e2853e23cf2f0dddb8b881b097d', u'_id': ObjectId('582cc1688726e30d949a9eab'), u'Pages': 1, u'Metadata': {u'/ModDate': u\"D:20150225112908-06'00'\", u'/GTS_PDFXVersion': u'PDF/X-4', u'/CreationDate': u\"D:20150206131811-06'00'\", u'/Producer': u'Adobe PDF Library 10.0.1', u'/Title': u'310-740-52.indd', u'/Creator': u'Adobe InDesign CS6 (Macintosh)', u'/Trapped': u'/False'}}\n"
     ]
    }
   ],
   "source": [
    "print(collection.find_one({\"Content_Hash\":spanish_content_hash[-1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "313\n",
      "[ 0.          0.          0.          0.          0.06688394  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.18101502\n",
      "  0.          0.04859089  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.12746195  0.          0.          0.\n",
      "  0.          0.11186231  0.          0.          0.          0.\n",
      "  0.06322667  0.          0.          0.          0.          0.\n",
      "  0.05629388  0.          0.          0.0633849   0.          0.          0.\n",
      "  0.          0.          0.0588243   0.          0.          0.          0.\n",
      "  0.05873524  0.          0.          0.          0.06110447  0.          0.\n",
      "  0.          0.23123829  0.          0.          0.49998184  0.          0.\n",
      "  0.          0.          0.          0.05820701  0.          0.          0.\n",
      "  0.10955114  0.          0.1352349   0.          0.          0.          0.\n",
      "  0.          0.18094425  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.06164421  0.          0.          0.          0.\n",
      "  0.          0.          0.06067082  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.05409424\n",
      "  0.          0.05477557  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.06330566  0.          0.\n",
      "  0.          0.04662657  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.12408741  0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.05545504  0.          0.          0.12383644  0.          0.          0.\n",
      "  0.          0.06057541  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.04673988  0.04620589  0.          0.\n",
      "  0.          0.          0.          0.06599447  0.          0.\n",
      "  0.06244926  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.19243405  0.          0.          0.\n",
      "  0.06250039  0.          0.          0.          0.          0.          0.\n",
      "  0.06530361  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.061669    0.          0.          0.          0.36691852\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.06944467  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.18404724  0.05662049  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.11222381  0.          0.26609838  0.          0.          0.          0.\n",
      "  0.          0.          0.05147939  0.          0.          0.          0.\n",
      "  0.13387706  0.          0.          0.          0.          0.06904948\n",
      "  0.          0.          0.          0.27665203  0.13648892  0.\n",
      "  0.06405446  0.          0.          0.          0.          0.          0.\n",
      "  0.06129948  0.          0.          0.12640077  0.05621285  0.          0.\n",
      "  0.06830798  0.09585969  0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "array_test = np.array(collection.find_one({\"Content_Hash\":english_content_hash[-1]})[\"TFIDF_Vector\"])\n",
    "print(type(array_test))\n",
    "print(len(array_test))\n",
    "print(array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3116\n",
      "440\n"
     ]
    }
   ],
   "source": [
    "cursor = collection.find({\"TFIDF_Vector\":{\"$exists\":False}})\n",
    "count = 0\n",
    "document_hashes = []\n",
    "for document in cursor:\n",
    "    try:\n",
    "        document_hashes.append(document['Hash'])\n",
    "    except:\n",
    "        continue\n",
    "print(len(document_hashes))\n",
    "print(len(set(document_hashes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33437\n",
      "5106\n"
     ]
    }
   ],
   "source": [
    "cursor = collection.find({\"TFIDF_Vector\":{\"$exists\":True}})\n",
    "count = 0\n",
    "document_hashes = []\n",
    "for document in cursor:\n",
    "    try:\n",
    "        document_hashes.append(document['Hash'])\n",
    "    except:\n",
    "        continue\n",
    "print(len(document_hashes))\n",
    "print(len(set(document_hashes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preserve TF-IDF Vectorizer(s) for use with future documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u\"Solid to the Core Providing Resources to Support Units, Chapters, and Lodges Every once in a while when you\\u2122re working on a pioneer ing project, you\\u2122ll find a spar that looks great but that turns out to be weak and unreliable. Maybe its center has been eaten away by insects. Or maybe it has natural splits inside that you can't see. You can test a spar fo r soundness by holding one end and rapping the other end sharply on a rock. If it\\u2122s sound you'll hear it ring. Otherwise, you\\u2122ll want to toss it aside and find a good, solid spar to work with. Some people are like defective spars. They look great on the outside and they may have appealing personalities, the kind of guys and girls you think you would like to know. But when you do get to know them better, you find that they\\u2122re like a defective spar\\u0160weak inside . They don\\u2122t have the strength of character to resist things that you know are wrong, and chances are they will want you to do those things, too. When that happens, do the same thing you do when you have a defective spar: Cast it aside and find a sounder one. In other words, choose friends who are solid to the core.\"]\n"
     ]
    }
   ],
   "source": [
    "test_document = english_text_corpus[0]\n",
    "#print(test_document)\n",
    "test_list = []\n",
    "test_list.append(test_document)\n",
    "print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alt_english_tfidf_vectorizer = TfidfVectorizer(max_df=0.8, \n",
    "                                           max_features=200000,\n",
    "                                           min_df=0.2, \n",
    "                                           stop_words=english_stopwords,\n",
    "                                           use_idf=True, \n",
    "                                           tokenizer=tokenize_and_stem_english, \n",
    "                                           ngram_range=(1,3),\n",
    "                                           vocabulary = collection.find_one({\"Document_Class\":\"english_resources\"})[\"TFIDF_Vocabulary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\tTrue\n",
      "  (0, 1)\tTrue\n",
      "  (0, 2)\tTrue\n",
      "  (0, 3)\tTrue\n",
      "  (0, 4)\tTrue\n",
      "  (0, 5)\tTrue\n",
      "  (0, 6)\tTrue\n",
      "  (0, 7)\tTrue\n",
      "  (0, 8)\tTrue\n",
      "  (0, 9)\tTrue\n",
      "  (0, 10)\tTrue\n",
      "  (0, 11)\tTrue\n",
      "  (0, 12)\tTrue\n",
      "  (0, 13)\tTrue\n",
      "  (0, 14)\tTrue\n",
      "  (0, 15)\tTrue\n",
      "  (0, 16)\tTrue\n",
      "  (0, 17)\tTrue\n",
      "  (0, 18)\tTrue\n",
      "  (0, 19)\tTrue\n",
      "  (0, 20)\tTrue\n",
      "  (0, 21)\tTrue\n",
      "  (0, 22)\tTrue\n",
      "  (0, 23)\tTrue\n",
      "  (0, 24)\tTrue\n",
      "  :\t:\n",
      "  (0, 283)\tTrue\n",
      "  (0, 284)\tTrue\n",
      "  (0, 286)\tTrue\n",
      "  (0, 287)\tTrue\n",
      "  (0, 288)\tTrue\n",
      "  (0, 290)\tTrue\n",
      "  (0, 291)\tTrue\n",
      "  (0, 292)\tTrue\n",
      "  (0, 293)\tTrue\n",
      "  (0, 294)\tTrue\n",
      "  (0, 295)\tTrue\n",
      "  (0, 296)\tTrue\n",
      "  (0, 297)\tTrue\n",
      "  (0, 298)\tTrue\n",
      "  (0, 300)\tTrue\n",
      "  (0, 301)\tTrue\n",
      "  (0, 302)\tTrue\n",
      "  (0, 303)\tTrue\n",
      "  (0, 304)\tTrue\n",
      "  (0, 305)\tTrue\n",
      "  (0, 306)\tTrue\n",
      "  (0, 308)\tTrue\n",
      "  (0, 310)\tTrue\n",
      "  (0, 311)\tTrue\n",
      "  (0, 312)\tTrue\n"
     ]
    }
   ],
   "source": [
    "test_tfidf_matrix = alt_english_tfidf_vectorizer.fit_transform(test_list)#english_text_corpus)\n",
    "#print(test_tfidf_matrix)\n",
    "print(test_tfidf_matrix == english_tfidf_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x167131ea0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collection.update_one({\"Document_Class\":\"resources\"},{\"$set\":{\"Document_Class\":\"english_resources\"}})\n",
    "#collection.update_one({\"Document_Class\":\"english_resources\"},{\"$set\":{\"TFIDF_Vocabulary\":english_terms}})\n",
    "#collection.update_one({\"Document_Class\":\"english_resources\"},{\"$set\":{\"existing_content_hash\":english_content_hash}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Document_Class', u'_id', u'TFIDF_Vocabulary', u'existing_content_hash']\n",
      "3979\n"
     ]
    }
   ],
   "source": [
    "new_test = collection.find_one({\"Document_Class\":\"english_resources\"})\n",
    "print(new_test.keys())\n",
    "print(len(new_test[\"existing_content_hash\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spanish_resource_object = {\"Document_Class\":\"spanish_resources\",\"TFIDF_Vocabulary\":spanish_terms}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertOneResult at 0x16712b168>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.insert_one(spanish_resource_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.UpdateResult at 0x16712b2d0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collection.update_one({\"Document_Class\":\"spanish_resources\"},{\"$set\":{\"TFIDF_Vocabulary\":spanish_terms}})\n",
    "collection.update_one({\"Document_Class\":\"spanish_resources\"},{\"$set\":{\"existing_content_hash\":spanish_content_hash}})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Document_Class', u'_id', u'TFIDF_Vocabulary', u'existing_content_hash']\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "result = collection.find_one({\"Document_Class\":\"spanish_resources\"})\n",
    "print(result.keys())\n",
    "print(len(result[\"existing_content_hash\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_io_test = StringIO()\n",
    "pickle.dump(english_dist, string_io_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'<'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-46b5a9d5bee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring_io_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ajwil\\Anaconda2\\lib\\pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\ajwil\\Anaconda2\\lib\\pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '<'"
     ]
    }
   ],
   "source": [
    "test_result = pickle.load(StringIO(string_io_test))\n",
    "print(type(test_result))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
