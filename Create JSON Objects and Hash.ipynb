{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 11:24:34.118000\n",
      "Failure on UID: BSA_3137\n",
      "Failure on UID: BSA_6862\n",
      "Failure on UID: BSA_11351\n",
      "Failure on UID: BSA_15165\n",
      "Failure on UID: BSA_20002\n",
      "Failure on UID: BSA_23981\n",
      "Failure on UID: BSA_28031\n",
      "Failure on UID: BSA_29405\n",
      "Failure on UID: BSA_29406\n",
      "Failure on UID: BSA_29407\n",
      "Failure on UID: BSA_29408\n",
      "Failure on UID: BSA_29409\n",
      "Failure on UID: BSA_29410\n",
      "Failure on UID: BSA_29411\n",
      "Failure on UID: BSA_29412\n",
      "Failure on UID: BSA_29413\n",
      "Failure on UID: BSA_29414\n",
      "Failure on UID: BSA_29415\n",
      "Failure on UID: BSA_29416\n",
      "Failure on UID: BSA_29417\n",
      "Failure on UID: BSA_29418\n",
      "Failure on UID: BSA_29419\n",
      "Failure on UID: BSA_29420\n",
      "Failure on UID: BSA_29421\n",
      "Failure on UID: BSA_29422\n",
      "Failure on UID: BSA_29423\n",
      "Failure on UID: BSA_32544\n",
      "Failure on UID: BSA_36640\n",
      "End: 11:50:18.106000\n"
     ]
    }
   ],
   "source": [
    "#Create JSON Objects for each and every PDF File within a new folder\n",
    "\n",
    "import PyPDF2\n",
    "import sys\n",
    "import collections\n",
    "import urllib2\n",
    "from StringIO import StringIO\n",
    "from django.utils.encoding import smart_str, smart_unicode\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import csv as csv\n",
    "from __future__ import print_function\n",
    "import hashlib\n",
    "import os\n",
    "import json\n",
    "\n",
    "start_time = datetime.datetime.now().time()\n",
    "print(\"Start: \"+str(start_time))\n",
    "\n",
    "#Step 1: Import table of UIDs and URLs to use as the starting point for the dictionaries\n",
    "urlref = {}\n",
    "urldf = pd.read_csv(\"UID_URL_List.txt\", sep='\\t')\n",
    "for x in range (0, len(urldf)):\n",
    "        urlref.update({urldf[\"UID\"][x]:urldf[\"URL\"][x]})\n",
    "\n",
    "#create a global dictionary of the UID/URL Pairs\n",
    "def createJSON(uid):\n",
    "    pdfDict = {} #pdfDict.update({'key':value})\n",
    "    temp_pdf = PyPDF2.PdfFileReader(file('D:\\BSA_PDF_Files\\\\'+uid+'.pdf',\"rb\"))\n",
    "    \n",
    "    #2.b. UID:String\n",
    "    pdfDict.update({'UID':uid})\n",
    "    \n",
    "    #2.c. URL:String\n",
    "    pdfDict.update({'URL':urlref[uid]})\n",
    "    \n",
    "    #2.d. metaData:Dictionary\n",
    "    pdfDict.update({'Metadata':temp_pdf.documentInfo})\n",
    "    \n",
    "    #2.e. fileSize:long\n",
    "    pdfDict.update({'File_Size': os.path.getsize('D:\\BSA_PDF_Files\\\\'+uid+'.pdf')})\n",
    "    \n",
    "    #2.f. hashedBin:String // Hashlib.sha1([pdf data here]).hexdigest() to create a hash value for unique idâ€™ing your docs\n",
    "    tempHash = hashlib.sha1(open('D:\\BSA_PDF_Files\\\\'+uid+'.pdf','rb').read()).hexdigest()\n",
    "    pdfDict.update({'Hash':tempHash})\n",
    "    #add has to global dictionary if not already present, append UID to the hash\n",
    "    global glbHashes\n",
    "    if tempHash in glbHashes:\n",
    "        glbHashes[tempHash].append(uid)\n",
    "    else:\n",
    "        glbHashes.update({tempHash:[uid]})\n",
    "    \n",
    "    \n",
    "    url = urlref[uid].strip().split('/')\n",
    "    #2.g. Domain:String\n",
    "    pdfDict.update({'Domain':url[2]})\n",
    "    \n",
    "    #2.i. Folder: String\n",
    "    pdfDict.update({'Folder':url[-2]})\n",
    "    \n",
    "    #2.j. fileName:String\n",
    "    pdfDict.update({'File_Name':url[-1]})\n",
    "    \n",
    "    with open('D:\\BSA_PDF_Files\\JSON\\\\'+uid+'.json','w') as j:\n",
    "        json.dump(pdfDict, j, indent = 4)\n",
    "\n",
    "        \n",
    "#Step 1: Import table of UIDs and URLs to use as the starting point for the dictionaries\n",
    "urlref = {}\n",
    "uids = []\n",
    "urldf = pd.read_csv(\"UID_URL_List.txt\", sep='\\t')\n",
    "for x in range (0, len(urldf)):\n",
    "        urlref.update({urldf[\"UID\"][x]:urldf[\"URL\"][x]})\n",
    "        uids.append(urldf[\"UID\"][x])\n",
    "\n",
    "glbHashes = {}\n",
    "#Step 2: Iterate through each and every UID/URL combo to create a new dictionary for that file containing:\n",
    "test_uids = ['BSA_101', 'BSA_102', 'BSA_103', 'BSA_104']\n",
    "for uid in uids:\n",
    "    try: createJSON(uid)\n",
    "    except: print(\"Failure on UID: \"+uid)\n",
    "#print(glbHashes) \n",
    "with open ('D:\\BSA_PDF_Files\\JSON\\hashMatching.json','w') as f:\n",
    "    json.dump(glbHashes, f, indent = 4)\n",
    "\n",
    "end_time = datetime.datetime.now().time()\n",
    "print(\"End: \"+str(end_time))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
